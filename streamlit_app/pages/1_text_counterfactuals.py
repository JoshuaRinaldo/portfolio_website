import streamlit as st

import base64
import boto3
import os
import json

lambda_client = boto3.client("lambda")

counterfactual_lambda = os.getenv("COUNTERFACTUAL_LAMBDA")
sentiment_unmasking_model = os.getenv("SENTIMENT_UNMASKING_MODEL")
sentiment_classification_model = os.getenv("SENTIMENT_CLASSIFICATION_MODEL")


greens = ["d6e6d5", "e1ffe0", "c8ffc7", "a8faa7", "90ff8f", "6bff69", "40ff3d", "07fc03"]
reds = ["e6d5d5", "facfcf", "ffbaba", "ffa1a1", "ff8585", "ff6666", "ff3d3d", "ff1919"]

def create_hex_css(explanation, desired_label, undesired_label):
    base_css = '<mark style="background-color: #{hex_color};">{text}</mark>'
    output_str = ""
    
    for token_explanation_n in explanation:
        token_n = token_explanation_n[0]
        collapsed_value = token_explanation_n[1][desired_label] - token_explanation_n[1][undesired_label]

        color_index = min(int(abs(collapsed_value)*8*2.5-1), 7)
        if collapsed_value > 0:
            color = greens[color_index]
        elif collapsed_value < 0:
            color = reds[color_index]
        else:
            color = "fcfcfc"

        token_css_string = base_css.format(hex_color=color, text=token_n)
        output_str += token_css_string

    return output_str

st.title("Text Counterfactuals")

file_ = open("static/text_counterfactual_animation.gif", "rb")
contents = file_.read()
data_url = base64.b64encode(contents).decode("utf-8")
file_.close()

st.markdown(
    f'''<p align="center">
    <img src="data:image/gif;base64,{data_url}" alt="A diagram showing a string being converted from positive to negative sentiment by replacing the word terrible with the word wonderful">
    </p>''',
    unsafe_allow_html=True,
)

st.markdown(
"""
## Welcome!

This is the webpage for my text counterfactuals project. It provides
access to live models which you can use to generate your own
counterfactuals.
"""
)

st.markdown("## Blog Post")

with st.expander("Blog Post"):

    st.markdown(
    """

    ## Introduction

    >_A counterfactual explanation describes a causal situation in the form: “If X had not occurred, Y would not have occurred.” For example: “If I hadn’t taken a sip of this hot coffee, I wouldn’t have burned my tongue.” Event Y is that I burned my tongue; cause X is that I had a hot coffee._

    > -_Susanne Dandl & Christoph Molnar, [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/counterfactual.html)_

    Understanding why something happened is important, equally as important
    is understanding what could've been done to reach an alternative
    outcome. In machine learning, counterfactuals provide insight into
    these "what if..?" questions we might have about our models.
    Counterfactuals are most readily available for tabular data.
    [DiCE](https://interpret.ml/DiCE/) provides a flexible
    library for this modality. While for tabular data we can select
    categories from a discrete, finite list, or alter a continuous
    numerical value to explore alternative outcomes, other modalities such
    as images, video, audio, and text provide unique challenges due to the
    complexity of finding suitable alternatives.

    ## Motivation

    I am interested in explainable AI, so I was pondering this question one
    day when it occurred to me that we could target important tokens by
    checking their Shapley values generated by the
    [SHAP](https://shap.readthedocs.io/en/latest/index.html) library, then
    mask and unmask those tokens using a masked language model. This solves
    the problem of finding suitable replacements in strings, where only a
    small fraction of words can be swapped while remaining syntactically
    correct. As it turns out, this technique already exists, but I thought
    it would be an interesting challenge to both attempt it and tackle the
    ML infrastructure challenges associated with generating these
    counterfactuals in (near) real time so they can be served to users.
    After all, what good is a cool feature if ML engineers are the only
    ones who can enjoy it?

    So I set upon this project with the goal of replicating the existing ML
    research and creating my own pipeline, with the added challenge of
    building something that could be served to users. As an added
    constraint, this system would need to be relatively inexpensive in
    production, since it is simply a personal project that will have low
    traffic. This meant that the counterfactual generation service would
    need to be serverless.

    ## Problems, Solutions

    **Problem: Inexpensively serving model explanations.** AWS, my cloud
    computing platform of choice, does not have an out-of-the-box solution
    to _serverlessly_ serve model explanations. AWS provides a service
    called [**SageMaker Clarify**](https://aws.amazon.com/sagemaker-ai/clarify/),
    which is essentially a wrapper for models that supports serving
    realtime model explanations. However, these realtime explanations can
    only be served by regular SageMaker endpoints, and there is no
    serverless option. This might not be a problem for an enterprise that
    has high traffic and large budgets, but since I am building a
    low-traffic demonstration for a personal project, this would be too
    expensive of a solution.

    **Solution: Build a custom container that uses SHAP to serve model
    explanations.** We can circumvent the limitation imposed by SageMaker
    Clarify by building the SHAP wrapper ourselves in a custom docker
    image, then serve that container as a "regular" endpoint serverlessly.
    The result of this is an endpoint that is capable of returning
    explanations exactly like a SageMaker Clarify endpoint would. One major
    limitation to this approach is that serverless endpoints have a maximum
    memory of `6GB` and cannot make use of GPUs. This means that we must
    minimize the size of our docker image and inference model as much as
    possible. Ideally, we should make use of smaller (distiled) transformer
    models to minimize latency and avoid overloading the memory of our
    serverless machine. Likewise. the docker image should be as small as
    possible.

    **Problem: The pipeline does not flip the predicted class.** A problem with masking
    and unmasking tokens in a string is that many tokens can contribute to
    a predicted class. If we remove too many tokens at once, the string
    begins to lose all meaning, and it is important that the input retains
    as much of its original meaning as possible. This means that we need to
    find a reasonable number of tokens to mask so we can change the label
    without completely altering the input. Too few masks will result in
    mangled sentences that don't make sense, and too many masks will result
    in counterfactuals that either don't or just barely flip the predicted
    class.

    **Solution: When needed, run input text through multiple mask-unmask
    cycles.**
    To avoid masking too many tokens at once, I set a cap on the number of
    tokens that could be masked in a single counterfactual generation
    attempt. This cap reduces the risk of the mask-unmask process
    overzealously masking tokens and completely changing the content of the
    text input. Once the string has been unmasked and a list of
    counterfactuals are generated, we only keep counterfactuals where the
    predicted label has a score that is greater that 0.8. If none of the
    counterfactuals have a score of 0.5, we select the highest scoring
    counterfactual, then re-run the pipeline. The maximum depth of
    recursion is set at 3 cycles to avoid an infinite loop, and if no
    viable counterfactuals are generated, the output it returned with a
    message explaining the failure. This recursive approach works well for
    odd inputs where the pipeline would normally fail.

    **Problem: The counterfactuals completely change the content of the
    original input.** When generating counterfactuals at first, the
    pipeline would occasionally change words that were important to the
    meaning of the input but had little bearing on the predicted class.
    This is a unique challenge, because how can we know which tokens are
    important to the meaning of an input, aside from sorting them based on
    their shapley values?

    **Solution: implement part-of-speech tagging with
    [SpaCy](https://spacy.io/usage/linguistic-features#pos-tagging) and
    prevent certain token types from being masked.** To avoid the erroneous
    masking and unmasking of important tokens, I used a small SpaCy model
    to restrict the types of tokens that could be masked. For sentiment,
    usually descriptive words or verbs drive the predicted class (e.g. "I
    hate Mondays"). I found that nouns tended to not significantly
    contribute to negative sentiment predictions, while descriptive words
    such as adjectives, adverbs, and verbs were common culprits. However,
    simply disallowing the masking of nouns is too heavy-handed of an
    approach, and the process lacks naunce. For example the input of "You
    are an idiot" is clearly negative, but it hinges entirely on the noun
    "idiot". To soften the approach, we add an exception that disallowed
    parts of speech can still be masked if they fall within 20$\%$ of the
    highest shapley value. This gives us enough flexibility to enforce the
    rule while still accounting for less common cases. Please note that
    this addition can be toggled as an optional argument, so the parts of
    speech that are or aren't allowed are not hard-coded, since some parts
    of speech might be more or less important depending on the
    classification task.

    ## Conclusion

    Overall, this was a fun and challenging project. It is something I have
    wanted to do for quite some time, and I am happy to have completed it.
    This project forced me to face some interesting techincal challenges
    when working towards serving text counterfactuals. Working within the
    limitations of serverless cloud computing was a good oppourtunity to
    test myself. In addition to that, it was fun to explore the ways text
    counterfactual generation could be optimized based on the mask selection
    criteria and the selection of high quality counterfactuals.
    """
    )


st.markdown("## Try it Yourself")
with st.expander("Try it yourself"):

    st.markdown("Enter a short phrase to generate counterfactuals for")
    input_text = st.text_input("")
    if len(input_text)>100:
        st.markdown(
            "Your input is too long. Your input must be fewer than 100 characters."
            )

    elif st.button("Press to generate counterfactuals"):

        # Invoke lambda function
        response = lambda_client.invoke(
            FunctionName=counterfactual_lambda,
            Payload=json.dumps(
                {
                    "input_text": input_text,
                    "classification_model": sentiment_classification_model,
                    "masked_language_model": sentiment_unmasking_model,
                    "desired_class": "positive",
                    "undesired_class": "negative",
                }
            )
        )
        payload = json.loads(response['Payload'].read())

        st.write(payload)

        explanation = payload["endpoint_response"]["explanation"]
        display_text = create_hex_css(explanation=explanation, desired_label="positive", undesired_label="negative")
        st.markdown(
        display_text,
        unsafe_allow_html=True,
        )
        st.markdown(payload["result"])


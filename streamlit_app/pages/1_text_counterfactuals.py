import streamlit as st

import base64
import boto3
import os
import json

lambda_client = boto3.client("lambda")

counterfactual_lambda = os.getenv("COUNTERFACTUAL_LAMBDA")
sentiment_unmasking_model = os.getenv("SENTIMENT_UNMASKING_MODEL")
sentiment_classification_model = os.getenv("SENTIMENT_CLASSIFICATION_MODEL")


greens = ["d6e6d5", "e1ffe0", "c8ffc7", "a8faa7", "90ff8f", "6bff69", "40ff3d", "07fc03"]
reds = ["e6d5d5", "facfcf", "ffbaba", "ffa1a1", "ff8585", "ff6666", "ff3d3d", "ff1919"]

def create_hex_css(explanation, desired_label, undesired_label):
    base_css = '<mark style="background-color: #{hex_color};">{text}</mark>'
    output_str = ""
    
    for token_explanation_n in explanation:
        token_n = token_explanation_n[0]
        collapsed_value = token_explanation_n[1][desired_label] - token_explanation_n[1][undesired_label]

        color_index = min(int(abs(collapsed_value)*8*2.5-1), 7)
        if collapsed_value > 0:
            color = greens[color_index]
        elif collapsed_value < 0:
            color = reds[color_index]
        else:
            color = "fcfcfc"

        token_css_string = base_css.format(hex_color=color, text=token_n)
        output_str += token_css_string

    return output_str

st.title("Text Counterfactuals")

file_ = open("static/text_counterfactual_animation.gif", "rb")
contents = file_.read()
data_url = base64.b64encode(contents).decode("utf-8")
file_.close()

st.markdown(
    f'''<p align="center">
    <img src="data:image/gif;base64,{data_url}" alt="A diagram showing a string being converted from positive to negative sentiment by replacing the word terrible with the word wonderful">
    </p>''',
    unsafe_allow_html=True,
)

st.markdown(
"""
## Welcome!

This is the webpage for my text counterfactuals project. It provides
access to live models which you can use to generate your own
counterfactuals.
"""
)

st.markdown("## Blog Post")

with st.expander("Blog Post"):

    st.markdown(
    """

    ## Introduction

    >_A counterfactual explanation describes a causal situation in the form: “If X had not occurred, Y would not have occurred.” For example: “If I hadn’t taken a sip of this hot coffee, I wouldn’t have burned my tongue.” Event Y is that I burned my tongue; cause X is that I had a hot coffee._

    > -_Susanne Dandl & Christoph Molnar, [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/counterfactual.html)_

    Understanding why something happened is important, equally as important
    is understanding what could've been done to reach an alternative
    outcome. In machine learning, counterfactuals provide insight into
    these "what if..?" questions we might have about our models.
    Counterfactuals are most readily available for tabular data.
    [DiCE](https://interpret.ml/DiCE/) provides a flexible
    library for this modality. While for tabular data we can select
    categories from a discrete, finite list, or alter a continuous
    numerical value to explore alternative outcomes, other modalities such
    as images, audio, video, and text provide unique challenges due to the
    complexity of finding reasonable yet similar alternatives.

    ## Motivation

    I am interested in explainable AI, so I was thinking about the
    challenge of generating counterfactuals for the more complex
    modalities one day. It occurred to me that we could identify
    important tokens by checking their Shapley values as generated by
    the [SHAP](https://shap.readthedocs.io/en/latest/index.html)
    library, then mask and unmask those tokens using a masked language
    model. This solves the problem of finding suitable replacements in
    strings, where only a small fraction of words can be swapped while
    remaining syntactically correct. As it turns out, this technique
    already exists, but I thought it would be an interesting challenge
    to both attempt it and tackle the ML infrastructure challenges
    associated with generating these counterfactuals in (near) real
    time so they can be served to users. After all, what good is a cool
    feature if ML engineers are the only ones who can enjoy it?

    So I set upon this project with the goal of replicating the existing ML
    research and creating my own pipeline, with the added challenge of
    building something that could be served to users. As an added
    constraint, this system would need to be relatively inexpensive in
    production, since it is simply a personal project that will have low
    traffic. This meant that the counterfactual generation service would
    need to be _serverless_.

    ## Problems, Solutions

    **Problem: Serving Model Explanations is Expensive.** AWS, my cloud
    computing platform of choice, does not have an out-of-the-box solution
    to _serverlessly_ serve model explanations. AWS provides a service
    called [**SageMaker Clarify**](https://aws.amazon.com/sagemaker-ai/clarify/),
    which is essentially a wrapper for models that supports serving
    realtime model explanations. However, these realtime explanations can
    only be served by regular SageMaker endpoints, and there is no
    serverless option. This might not be a problem for an enterprise that
    has high traffic and large budgets, but since I am building a
    low-traffic demonstration for a personal project, this would be too
    expensive of a solution.

    **Solution: Build a custom container that uses SHAP to serve model
    explanations.** We can circumvent the limitation imposed by SageMaker
    Clarify by building a SHAP wrapper ourselves in a custom docker
    image, then serve that container as a serverless endpoint. This
    solution is nice, because it allows SageMaker Clarify like
    functionality without being tied to a specific platform, which
    makes local testing much easier and could be used in other cloud'
    platforms.
    
    **Problem: Serverless Endpoints are Small**
    On SageMaker, serverless endpoints have a maximum memory of `6GB`
    and cannot use GPUs. This means that we must minimize the size of
    the docker image and inference model.

    **Solution: Use Smaller Models, Expect Occasional Throttling**
    Sometimes the biggest hammer isn't the right one for the job. While
    a larger model will probably result in better performance, it is
    often at the cost of being larger and more computationally taxing.
    The first approach to this problem is to use smaller transformer
    models. The masked language model is a finetuned version of
    distilbert, and it works well. However, using distilbert for the
    sentiment classifier yielded poor results, especially complex
    classification tasks. Identifying which words to mask is a critical
    part of the counterfactual generation pipeline, and the process
    relies on a strong classification model, so I chose to use roBERTa,
    a model that is rather large. This large model works well most of
    the time, but for complicated or difficult counterfactuals, it will
    sometimes begin to throttle. To handle this, a retry loop is built
    in to the endpoint invocation function.

    **Problem: The pipeline does not flip the predicted class.** A
    problem with masking and unmasking tokens in a string is that many
    tokens can contribute to a predicted class. If we remove too many
    tokens at once, the input begins to change in content completely.
    The goal of counterfactuals is to find what minor changes can be
    made to alter the class, so we should strive to minimize the
    changes made to the original input. We must find a reasonable
    number of tokens to mask so we can change the label without
    completely altering the input. Too few masks can result in mangled
    sentences that don't make sense or fail to flip the predicted
    class, while too many masks can result in counterfactuals that
    barely resemble the original input.

    **Solution: When needed, run input text through multiple
    mask-unmask cycles.** To avoid masking too many tokens at once, I
    set a cap on the number of tokens that could be masked in a single
    counterfactual generation attempt. This cap reduces the risk of the
    mask-unmask process overzealously masking tokens and completely
    changing the content of the input. Once the tokens have been
    unmasked and a list of counterfactuals are generated, we only keep
    counterfactuals where the predicted label (of our desired class)
    has a score that is greater than 0.7. If none of the
    counterfactuals have a score of 0.7, we select the highest scoring
    counterfactual, then re-run the pipeline. The maximum depth of
    recursion is set at 3 cycles to avoid an infinite loop, and if no
    viable counterfactuals are generated, the output is returned with a
    message explaining the failure. This recursive approach works well for
    strange inputs where the pipeline would normally fail.

    **Problem: The counterfactuals completely change the content of the
    original input.** When generating counterfactuals at first, the
    pipeline would occasionally change words that were important to the
    meaning of the input but had a relatively small impact on the
    predicted class. This is a unique challenge, because how can we
    know which tokens are important to the meaning of an input, aside
    from sorting them based on their shapley values?

    **Solution: implement part-of-speech tagging with
    [SpaCy](https://spacy.io/usage/linguistic-features#pos-tagging) and
    prevent certain token types from being masked.** To avoid the
    erroneous masking and unmasking of important tokens, I used a small
    SpaCy model to restrict the types of tokens that could be masked.
    For sentiment, usually descriptive words or verbs drive the
    predicted class (e.g. "I hate Mondays"). While nouns tended to not
    significantly contribute to negative sentiment predictions, but
    were important to the meaning of the original input. However,
    simply disallowing the masking of nouns is too heavy-handed of an
    approach, and the process lacks the nuance that is inherent in
    language. For example, the input of "You are an idiot" is clearly
    negative, but it hinges entirely on the noun "idiot". To soften the
    approach, we add an exception that disallowed parts of speech can
    still be masked if they fall within 20$\%$ of the highest shapley
    value for that input. This gives us enough flexibility to enforce
    the rule while still accounting for less common cases. Please note
    that which parts of speech are disallowed can be toggled as an
    optional argument, since some parts of speech might be more or less
    important depending on the classification task.

    ## Conclusion

    Overall, this was a fun and challenging project. It is something I
    have wanted to do for quite some time, and I am happy to have
    completed it. This project forced me to face some interesting
    technical challenges. Working within the limitations of serverless
    cloud computing was a good opportunity to test myself. In addition
    to that, it was fun to explore the ways text counterfactual
    generation could be optimized based on the mask selection criteria
    and the selection of high quality counterfactuals.
    """
    )


st.markdown("## Try it Yourself")
with st.expander("Try it yourself"):

    st.markdown("Enter a short phrase to generate counterfactuals for")
    input_text = st.text_input("")
    if len(input_text)>100:
        st.markdown(
            "Your input is too long. Your input must be fewer than 100 characters."
            )

    elif st.button("Press to generate counterfactuals"):

        # Invoke lambda function
        response = lambda_client.invoke(
            FunctionName=counterfactual_lambda,
            Payload=json.dumps(
                {
                    "input_text": input_text,
                    "classification_model": sentiment_classification_model,
                    "masked_language_model": sentiment_unmasking_model,
                    "desired_class": "positive",
                    "undesired_class": "negative",
                }
            )
        )
        payload = json.loads(response['Payload'].read())

        st.write(payload)

        explanation = payload["endpoint_response"]["explanation"]
        display_text = create_hex_css(explanation=explanation, desired_label="positive", undesired_label="negative")
        st.markdown(
        display_text,
        unsafe_allow_html=True,
        )
        st.markdown(payload["result"])

